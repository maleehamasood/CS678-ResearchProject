Arguments:
	   batch_size : 10
	      dataset : synthetic_1_1
	 drop_percent : 0.3
	   eval_every : 1
	learning_rate : 0.01
	        model : mclr
	 model_params : (10,)
	           mu : 0
	   num_epochs : 20
	    num_iters : 1
	   num_rounds : 200
	    optimizer : fedavg
	         seed : 0
Using Federated avg to Train

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/1.50k flops)
  dense/kernel/Regularizer/l2_regularizer (1/900 flops)
    dense/kernel/Regularizer/l2_regularizer/L2Loss (899/899 flops)
  dense/kernel/Initializer/random_uniform (300/601 flops)
    dense/kernel/Initializer/random_uniform/mul (300/300 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.00k flops)
  dense/kernel/Regularizer/l2_regularizer (1/1.80k flops)
    dense/kernel/Regularizer/l2_regularizer/L2Loss (1.80k/1.80k flops)
  dense/kernel/Initializer/random_uniform (600/1.20k flops)
    dense/kernel/Initializer/random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
10 Clients in Total
7 High End Clients, 3 Low End Clients
optionss {'optimizer': 'fedavg', 'dataset': 'synthetic_1_1', 'model': 'mclr', 'num_rounds': 200, 'eval_every': 1, 'batch_size': 10, 'num_epochs': 20, 'num_iters': 1, 'learning_rate': 0.01, 'mu': 0, 'seed': 0, 'drop_percent': 0.3, 'model_params': (10,), 'num_features': 60}
Training with 10 workers ---
At round 0 accuracy: 0.035977859778597784
At round 0 training accuracy: 0.034791666666666665
At round 0 training loss: 6.86619668846329
At round 1 accuracy: 0.07195571955719557
At round 1 training accuracy: 0.06802083333333334
At round 1 training loss: 5.233179699207346
At round 2 accuracy: 0.11623616236162361
At round 2 training accuracy: 0.115
At round 2 training loss: 4.89732758491921
At round 3 accuracy: 0.4455719557195572
At round 3 training accuracy: 0.473125
At round 3 training loss: 1.4661736610593896
At round 4 accuracy: 0.40682656826568264
At round 4 training accuracy: 0.4334375
At round 4 training loss: 1.7250643222406508
