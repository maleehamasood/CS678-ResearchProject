Arguments:
	       batch_size : 10
	clients_per_round : 5
	          dataset : synthetic_1_1
	     drop_percent : 0.7
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedavg
	             seed : 0
Using Federated avg to Train

WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/1.50k flops)
  dense/kernel/Regularizer/l2_regularizer (1/900 flops)
    dense/kernel/Regularizer/l2_regularizer/L2Loss (899/899 flops)
  dense/kernel/Initializer/random_uniform (300/601 flops)
    dense/kernel/Initializer/random_uniform/mul (300/300 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/3.00k flops)
  dense/kernel/Regularizer/l2_regularizer (1/1.80k flops)
    dense/kernel/Regularizer/l2_regularizer/L2Loss (1.80k/1.80k flops)
  dense/kernel/Initializer/random_uniform (600/1.20k flops)
    dense/kernel/Initializer/random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
k 1
k 8
k 2
k 24
k 13
k 8
k 3
k 26
k 28
k 27
k 1
k 12
k 17
k 17
k 10
10 Clients in Total
optionss {'optimizer': 'fedavg', 'dataset': 'synthetic_1_1', 'model': 'mclr', 'num_rounds': 200, 'eval_every': 1, 'clients_per_round': 5, 'batch_size': 10, 'num_epochs': 20, 'num_iters': 1, 'learning_rate': 0.01, 'mu': 0, 'seed': 0, 'drop_percent': 0.7, 'model_params': (10,)}
Training with 5 workers ---
At round 0 accuracy: 0.03505535055350553
At round 0 training accuracy: 0.0334375
At round 0 training loss: 6.838641936232646
At round 1 accuracy: 0.08118081180811808
At round 1 training accuracy: 0.08510416666666666
At round 1 training loss: 4.251157237873413
At round 2 accuracy: 0.06365313653136531
At round 2 training accuracy: 0.06760416666666667
At round 2 training loss: 3.4169006849701207
At round 3 accuracy: 0.46494464944649444
At round 3 training accuracy: 0.5114583333333333
At round 3 training loss: 1.3339061878683667
At round 4 accuracy: 0.38837638376383765
At round 4 training accuracy: 0.41041666666666665
At round 4 training loss: 1.6391332690076281
At round 5 accuracy: 0.507380073800738
At round 5 training accuracy: 0.5455208333333333
At round 5 training loss: 1.289256583241125
At round 6 accuracy: 0.3800738007380074
At round 6 training accuracy: 0.40989583333333335
At round 6 training loss: 1.9604489185785254
At round 7 accuracy: 0.42435424354243545
At round 7 training accuracy: 0.4423958333333333
At round 7 training loss: 1.5066144307733824
At round 8 accuracy: 0.41697416974169743
At round 8 training accuracy: 0.448125
At round 8 training loss: 1.9706199931186468
At round 9 accuracy: 0.12546125461254612
At round 9 training accuracy: 0.13520833333333335
At round 9 training loss: 2.262864394852271
At round 10 accuracy: 0.5525830258302583
At round 10 training accuracy: 0.5898958333333333
At round 10 training loss: 1.2431275852117687
At round 11 accuracy: 0.40498154981549817
At round 11 training accuracy: 0.4271875
At round 11 training loss: 1.6396278027746787
At round 12 accuracy: 0.40129151291512916
At round 12 training accuracy: 0.4276041666666667
At round 12 training loss: 1.8231906875098745
At round 13 accuracy: 0.3865313653136531
At round 13 training accuracy: 0.42083333333333334
At round 13 training loss: 2.34687651719898
At round 14 accuracy: 0.10977859778597786
At round 14 training accuracy: 0.12145833333333333
At round 14 training loss: 2.5398867062316275
At round 15 accuracy: 0.5369003690036901
At round 15 training accuracy: 0.530625
At round 15 training loss: 1.3259822756191715
At round 16 accuracy: 0.41420664206642066
At round 16 training accuracy: 0.4421875
At round 16 training loss: 1.419350903549542
At round 17 accuracy: 0.41697416974169743
At round 17 training accuracy: 0.4376041666666667
At round 17 training loss: 1.4298786566965282
At round 18 accuracy: 0.41974169741697415
At round 18 training accuracy: 0.4501041666666667
At round 18 training loss: 1.8524947981722653
At round 19 accuracy: 0.08856088560885608
At round 19 training accuracy: 0.0903125
At round 19 training loss: 2.019292340885537
At round 20 accuracy: 0.4215867158671587
At round 20 training accuracy: 0.44729166666666664
At round 20 training loss: 1.881610670791318
At round 21 accuracy: 0.1466789667896679
At round 21 training accuracy: 0.15572916666666667
At round 21 training loss: 2.4346037862698235
At round 22 accuracy: 0.12915129151291513
At round 22 training accuracy: 0.12791666666666668
At round 22 training loss: 2.666317256023176
At round 23 accuracy: 0.3367158671586716
At round 23 training accuracy: 0.36291666666666667
At round 23 training loss: 2.785478673639882
At round 24 accuracy: 0.5359778597785978
At round 24 training accuracy: 0.5725
At round 24 training loss: 1.1461839435777317
At round 25 accuracy: 0.3966789667896679
At round 25 training accuracy: 0.42354166666666665
At round 25 training loss: 1.7897625811677427
At round 26 accuracy: 0.5608856088560885
At round 26 training accuracy: 0.5945833333333334
At round 26 training loss: 1.1931339841801674
At round 27 accuracy: 0.47878228782287824
At round 27 training accuracy: 0.5077083333333333
At round 27 training loss: 1.3155393436861536
At round 28 accuracy: 0.4059040590405904
At round 28 training accuracy: 0.42583333333333334
At round 28 training loss: 1.481465989773472
At round 29 accuracy: 0.39391143911439114
At round 29 training accuracy: 0.4234375
At round 29 training loss: 1.762631371584915
At round 30 accuracy: 0.1992619926199262
At round 30 training accuracy: 0.22166666666666668
At round 30 training loss: 2.0506072790858645
At round 31 accuracy: 0.46586715867158673
At round 31 training accuracy: 0.4947916666666667
At round 31 training loss: 1.297374909352511
At round 32 accuracy: 0.42435424354243545
At round 32 training accuracy: 0.45166666666666666
At round 32 training loss: 1.8383757441739241
At round 33 accuracy: 0.551660516605166
At round 33 training accuracy: 0.5941666666666666
At round 33 training loss: 1.2032612762165567
At round 34 accuracy: 0.5359778597785978
At round 34 training accuracy: 0.5785416666666666
At round 34 training loss: 1.2892661465518176
At round 35 accuracy: 0.5691881918819188
At round 35 training accuracy: 0.5873958333333333
At round 35 training loss: 1.4638408922590316
At round 36 accuracy: 0.4714022140221402
At round 36 training accuracy: 0.498125
At round 36 training loss: 1.3961125106861194
At round 37 accuracy: 0.5396678966789668
At round 37 training accuracy: 0.5833333333333334
At round 37 training loss: 1.341121604774768
At round 38 accuracy: 0.4003690036900369
At round 38 training accuracy: 0.42885416666666665
At round 38 training loss: 2.3260032396639385
At round 39 accuracy: 0.544280442804428
At round 39 training accuracy: 0.5867708333333334
At round 39 training loss: 1.3071269043597082
At round 40 accuracy: 0.3856088560885609
At round 40 training accuracy: 0.41479166666666667
At round 40 training loss: 2.635979386276255
At round 41 accuracy: 0.5055350553505535
At round 41 training accuracy: 0.5328125
At round 41 training loss: 1.4951180525613017
At round 42 accuracy: 0.41420664206642066
At round 42 training accuracy: 0.43635416666666665
At round 42 training loss: 1.671178381294012
At round 43 accuracy: 0.5212177121771218
At round 43 training accuracy: 0.5430208333333333
At round 43 training loss: 1.2905023927117387
At round 44 accuracy: 0.4944649446494465
At round 44 training accuracy: 0.5151041666666667
At round 44 training loss: 1.4560714962923278
At round 45 accuracy: 0.40313653136531363
At round 45 training accuracy: 0.43697916666666664
At round 45 training loss: 2.025547906716141
At round 46 accuracy: 0.38468634686346864
At round 46 training accuracy: 0.4140625
At round 46 training loss: 2.392271781998376
At round 47 accuracy: 0.5
At round 47 training accuracy: 0.5373958333333333
At round 47 training loss: 1.4125107846750566
At round 48 accuracy: 0.39022140221402213
At round 48 training accuracy: 0.4215625
At round 48 training loss: 2.1821096555143593
At round 49 accuracy: 0.3856088560885609
At round 49 training accuracy: 0.41822916666666665
At round 49 training loss: 2.2434291114658116
At round 50 accuracy: 0.5359778597785978
At round 50 training accuracy: 0.578125
At round 50 training loss: 1.3923745945189148
At round 51 accuracy: 0.40682656826568264
At round 51 training accuracy: 0.42875
At round 51 training loss: 1.868605546184505
At round 52 accuracy: 0.3985239852398524
At round 52 training accuracy: 0.4276041666666667
At round 52 training loss: 2.769530550017953
At round 53 accuracy: 0.37915129151291516
At round 53 training accuracy: 0.4119791666666667
At round 53 training loss: 2.8958441371408603
At round 54 accuracy: 0.44280442804428044
At round 54 training accuracy: 0.4640625
At round 54 training loss: 2.616470319442451
At round 55 accuracy: 0.4547970479704797
At round 55 training accuracy: 0.47479166666666667
At round 55 training loss: 2.294737962788592
At round 56 accuracy: 0.4086715867158672
At round 56 training accuracy: 0.42916666666666664
At round 56 training loss: 1.840781177096069
At round 57 accuracy: 0.1070110701107011
At round 57 training accuracy: 0.11177083333333333
At round 57 training loss: 2.4092967198664943
